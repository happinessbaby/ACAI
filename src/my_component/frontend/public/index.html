<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Streamlit Component</title>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta name="description" content="Streamlit Component" />
    <base href="http://localhost:3001/">
    <link rel="stylesheet" href="bootstrap.min.css" />
    <script
    type="text/javascript"
    src="https://sdk.amazonaws.com/js/aws-sdk-2.645.0.min.js"
    ></script>
    <!--Three.js dependencies-->
    <script src="https://cdn.jsdelivr.net/npm/three@0.127.0/build/three.min.js"></script>
    <!--Host build file-->
    <!-- <script type="text/javascript" src="../node_module/@amazon-sumerian-hosts/three/dist/host.three.js"></script> -->
    <script type="text/javascript" src="host.three.js"></script>
    <script src="https://code.jquery.com/jquery-3.6.4.min.js"></script>
    <style>
      html,
      body {
        width: 100%;
        height: 100%;
        padding: 0;
        margin: 0;
        overflow: hidden;
        /* position:relative; */
      }
      .tab {
        display:none;
        /* background-color: rgb(219, 219, 219);
        padding-bottom: 0px;
        margin-bottom: -1px;
        border-width: 1px;
        border-style: solid;
        z-index: 2;
        position: relative;
        outline: 0px; */
      }

      .current {
        background-color: white;
        border-bottom-color: white;
        font-weight: bold;
      }

      .textEntry {
        display: none;
        /* min-width: 305px;
        min-height: 200px;
        outline: 0px;
        padding: 10px;
        resize: both; */
      }

      .recButton {
        display: block;
        width: 60px;
        height: 60px;
        font-size: 16px;
        font-weight: bold;
        color:white;
        background-color: red;
        border: 0;
        border-radius: 35px;
        margin: 18px;
        position: fixed;
        top: 70%; /* Center the loader vertically */
        left: 50%; /* Center the loader horizontally */
        margin-top: -60px; /* Adjust the negative margin to half of the loader height */
        margin-left: -60px; /* Adjust the negative margin to half of the loader width */
        outline:none
      }

      .notRec{
        background-color: darkred;
      }

      .Rec{
        animation-name: pulse;
        animation-duration: 1.5s;
        animation-iteration-count: infinite;
        animation-timing-function: linear;
      }

      @keyframes pulse{
        0%{
          box-shadow: 0px 0px 5px 0px rgba(173,0,0,.3);
        }
        65%{
          box-shadow: 0px 0px 5px 13px rgba(173,0,0,.3);
        }
        90%{
          box-shadow: 0px 0px 5px 13px rgba(173,0,0,0);
        }
      }
      .speechButton {
        width: 78.75px;
      }

      .gestureButton {
        width: 327px;
        outline: 0px;
      }

      .hidden {
        display: none;
      }

      #renderCanvas {
        width: 100%;
        height: 100%;
        touch-action: none;
      }

      #textToSpeech {
        padding: 2.5px;
        position: fixed;
        top: 0;
        left: 0;
        display: none;
      }
      .base-timer.label {
        position: fixed;   
        width: 50px;
        height: 50px;  
        color:white;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 50px;
        top: 20%; /* Center the loader vertically */
        left: 90%; /* Center the loader horizontally */
      }
  
      #loadScreen {
        display: block;
        align-items: center;
        justify-content: center;
        width: 100%;
        height: 100%;
        background-image: url('assets/images/teddy.jpeg');
        background-color: white;
        background-repeat: no-repeat;
        background-attachment: fixed;
        background-position: center;
        background-size: contain;
        z-index: 9999;
      }
      #loader {
      border: 16px solid #3498db38;
      border-radius: 50%;
      border-top: 16px solid #3498db;
      width: 120px;
      height: 120px;
      -webkit-animation: spin 2s linear infinite;
      animation: spin 2s linear infinite;
      position: fixed;
      top: 50%; /* Center the loader vertically */
      left: 50%; /* Center the loader horizontally */
      margin-top: -60px; /* Adjust the negative margin to half of the loader height */
      margin-left: -60px; /* Adjust the negative margin to half of the loader width */
    }
      @-webkit-keyframes spin {
      0% {
        -webkit-transform: rotate(0deg);
      }
      100% {
        -webkit-transform: rotate(360deg);
      }
    }

    @keyframes spin {
      0% {
        transform: rotate(0deg);
      }
      100% {
        transform: rotate(360deg);
      }
    }
    </style>
  </head>
  <body>
    <div id="root"></div>

 <!--Loading screen-->
 <div id="loadScreen">
  <div id="loader"></div>
</div>

<!--Text to speech controls-->
<div id="textToSpeech">
  <!-- <button class="tab current">Luke</button>
  <button class="tab">Alien</button> -->
  <div class="tab current"></div>
  <div class="tab"></div>
  <div>
    <textarea autofocus size="23" type="text" class="textEntry Interviewer">
<speak>
  <!-- <amazon:domain name="conversational">
    Hello, my name is Luke. I used to only be a host inside Amazon Sumerian, but
    now you can use me in other Javascript runtime environments like three js. Right now, 
    <mark name='{"feature":"PointOfInterestFeature","method":"setTargetByName","args":["chargaze"]}'/> 
    my friend and I here are in three js.
  </amazon:domain> -->
</speak>
    </textarea>
    <textarea autofocus size="23" type="text" class="textEntry Grader">
<speak id="speech_Alien">
</speak>
    </textarea>
  </div>
  <!-- <div>
    <button id="play" class="speechButton">Play</button>
    <button id="pause" class="speechButton">Pause</button>
    <button id="resume" class="speechButton">Resume</button>
    <button id="stop" class="speechButton">Stop</button>
  </div> -->
  <!-- <div>
    <button id="gestures" class="gestureButton">Generate Gestures</button>
  </div>
  <div>
    <select id="emotes" class="gestureButton"></select>
  </div>
  <div>
    <button id="playEmote" class="gestureButton">Play Emote</button>
  </div> -->
</div>
<div>
  <button id="skip-intro" class="skipIntroButton">Skip intro</button>
  <button id="recDisplay" class="recButton hidden">REC</button> 
  <span id="base-timer-label" class="base-timer label hidden"></span>
  <button id="stop-interview" class="stopInterviewButton hidden">stop interview</button>
</div>
<script type="module">

  import { OrbitControls } from 'https://cdn.jsdelivr.net/npm/three@0.127.0/examples/jsm/controls/OrbitControls.js';
  import { GLTFLoader } from 'https://cdn.jsdelivr.net/npm/three@0.127.0/examples/jsm/loaders/GLTFLoader.js';
  import { init_audioRecorder} from "./AudioRecorder.mjs";
  import {init_audioReceiver} from "./AudioReceiver.mjs"

  // Initialize host and greeting
  var host_name = "Luke";
  var greeting = 'Hello welcome to your interview. Are you ready to get started?';
  try {
      var storedGreeting = localStorage.getItem('greetingData');
      console.log("got greeting data", JSON.parse(storedGreeting))
      var greetingData = storedGreeting ? JSON.parse(storedGreeting) : {};
      host_name = greetingData.name || "Luke";
      greeting = greetingData.greeting || 'Hello welcome to your interview. Are you ready to get started?';
  }
  catch(err)
  {
    console.log(err)
  }


  const renderFn = [];
  const speakers = new Map([
    [host_name, undefined],
    ['Alien', undefined],
  ]);
  const intro_settings = new Map([
    ["scene", undefined],
    ["camera", undefined],
    ["clock", undefined],
    ["interviewer_chara", undefined],
    ["grader_chara", undefined],
    ["interviewer_host", undefined],
    ["grader_host", undefined],
  ])
  var count = 0;
  var finished_intro=false;
  var during_interview=false;
  let timerInterval;
  let countdownInterval;
  let characterData1;
  let characterData2;
  // let scene, camera, clock;
  const voiceEngine = 'neural';
  setCurrentHost(host_name);

  const audioRecorder = init_audioRecorder();
  init_audioReceiver();
  
  // document.addEventListener('DOMContentLoaded', function() {
      // init_audioReceiver();
    // });

 

  /* When the openFullscreen() function is executed, open the video in fullscreen.
Note that we must include prefixes for different browsers, as they don't support the requestFullscreen property yet */
function openFullscreen() {
  var elem = document.documentElement;
  if (elem.requestFullscreen) {
    elem.requestFullscreen();
  } else if (elem.webkitRequestFullscreen) { /* Safari */
    elem.webkitRequestFullscreen();
  } else if (elem.msRequestFullscreen) { /* IE11 */
    elem.msRequestFullscreen();
  }
}

document.onkeypress = function (e) {
    e = e || window.event;
    if (e.keyCode == 13) {
         openFullscreen();
         main();
         return true;
      } else {
         return false;
      }
}



  async function main() {

    // This is served by webpack-dev-server and comes from demo-credentials.js in the repo root
    // const config = await(await fetch("/devConfig.json")).json();
    // // Parse the region out of the cognito Id
    // const region = config.cognitoIdentityPoolId.split(":")[0];


    // Initialize AWS and create Polly service objects
    window.AWS.config.region = 'us-east-1';
    window.AWS.config.credentials = new AWS.CognitoIdentityCredentials({
      IdentityPoolId:'us-east-1:1af97f5a-3988-4abc-bb7d-19fc2005de7a',
    });
    const polly = new AWS.Polly();
    const presigner = new AWS.Polly.Presigner();
    const speechInit = HOST.aws.TextToSpeechFeature.initializeService(
      polly, 
      presigner,
      window.AWS.VERSION
    );


    characterData1 = await getCharacterData(host_name);
    characterData2 = await getCharacterData("Alien");
    

    // Set up the scene and host
    const {scene, camera, clock} = createScene();
    intro_settings.set("scene", scene);
    intro_settings.set("camera", camera);
    intro_settings.set("clock", clock);
    // save scene, camera, and clock

    const {
      character: character1,
      clips: clips1,
      bindPoseOffset: bindPoseOffset1,
    } = await loadCharacter(
      scene,
      characterData1.characterFile,
      characterData1.animationPath,
      characterData1.animationFiles
    );
    intro_settings.set("interviewer_chara", character1)
    const {
      character: character2,
      clips: clips2,
      bindPoseOffset: bindPoseOffset2,
    } = await loadCharacter(
      scene,
      characterData2.characterFile,
      characterData2.animationPath,
      characterData2.animationFiles
    );
    intro_settings.set("grader_chara", character2)

    character1.position.set(1.25, 0, 0);
    character1.rotateY(-0.5);
    character2.position.set(-0.5, 0, 0);
    character2.rotateY(0.5);

    // Find the joints defined by name
    const audioAttach1 = character1.getObjectByName(characterData1.audioAttachJoint);
    const audioAttach2 = character2.getObjectByName(characterData2.audioAttachJoint);
    const lookTracker1 = character1.getObjectByName(characterData1.lookJoint);
    const lookTracker2 = character2.getObjectByName(characterData2.lookJoint);

    // Read the gesture config file. This file contains options for splitting up
    // each animation in gestures.glb into 3 sub-animations and initializing them
    // as a QueueState animation.
    const gestureConfig1 = await fetch(
      `${characterData1.animationPath}/${characterData1.gestureConfigFile}`
    ).then(response => response.json());
    const gestureConfig2 = await fetch(
      `${characterData2.animationPath}/${characterData2.gestureConfigFile}`
    ).then(response => response.json());

    // Read the point of interest config file. This file contains options for
    // creating Blend2dStates from look pose clips and initializing look layers
    // on the PointOfInterestFeature.
    const poiConfig1 = await fetch(
      `${characterData1.animationPath}/${characterData1.poiConfigFile}`
    ).then(response => response.json());
    const poiConfig2 = await fetch(
      `${characterData2.animationPath}/${characterData2.poiConfigFile}`
    ).then(response => response.json());

    const [
      idleClips1,
      lipsyncClips1,
      gestureClips1,
      emoteClips1,
      faceClips1,
      blinkClips1,
      poiClips1,
    ] = clips1;
    const host1 = createHost(
      character1,
      audioAttach1,
      characterData1.voice,
      voiceEngine,
      idleClips1[0],
      faceClips1[0],
      lipsyncClips1,
      gestureClips1,
      gestureConfig1,
      emoteClips1,
      blinkClips1,
      poiClips1,
      poiConfig1,
      lookTracker1,
      bindPoseOffset1,
      clock,
      camera,
      scene
    );
    intro_settings.set("interviewer_host", host1)
    const [
      idleClips2,
      lipsyncClips2,
      gestureClips2,
      emoteClips2,
      faceClips2,
      blinkClips2,
      poiClips2,
    ] = clips2;
    const host2 = createHost(
      character2,
      audioAttach2,
      characterData2.voice,
      voiceEngine,
      idleClips2[0],
      faceClips2[0],
      lipsyncClips2,
      gestureClips2,
      gestureConfig2,
      emoteClips2,
      blinkClips2,
      poiClips2,
      poiConfig2,
      lookTracker2,
      bindPoseOffset2,
      clock,
      camera,
      scene
    );
    intro_settings.set("grader_host", host2)
  
    // Set up each host to look at the other when the other speaks and at the
    // camera when speech ends
    const onHost1StartSpeech = () => {
      setCurrentHost(host_name);
      host2.PointOfInterestFeature.setTarget(lookTracker1);
    };
    const onHost2StartSpeech = () => {
      setCurrentHost("Alien");
      host1.PointOfInterestFeature.setTarget(lookTracker2);
    };
    const onStopSpeech = () => {
      host1.PointOfInterestFeature.setTarget(camera);
      host2.PointOfInterestFeature.setTarget(camera);
    };

    host1.listenTo(
      host1.TextToSpeechFeature.EVENTS.play,
      onHost1StartSpeech
    );
    host1.listenTo(
      host1.TextToSpeechFeature.EVENTS.resume,
      onHost1StartSpeech
    );
    host2.listenTo(
      host2.TextToSpeechFeature.EVENTS.play,
      onHost2StartSpeech
    );
    host2.listenTo(
      host2.TextToSpeechFeature.EVENTS.resume,
      onHost2StartSpeech
    );
    HOST.aws.TextToSpeechFeature.listenTo(
      HOST.aws.TextToSpeechFeature.EVENTS.pause,
      onStopSpeech
    );
    HOST.aws.TextToSpeechFeature.listenTo(
      HOST.aws.TextToSpeechFeature.EVENTS.stop,
      onStopSpeech
    );

    // Hide the load screen and show the text input
    document.getElementById('textToSpeech').style.display = 'inline-block';
    document.getElementById('loadScreen').style.display = 'none';

    await speechInit;

    speakers.set(host_name, host1);
    speakers.set('Alien', host2);

    initializeIntro();

  }

  function initializeIntro() {

    const grader_greeting =  "Hi, I'm Ali. I'll be supervising your interview and providing feedback at the end of the session. Good Luck!"

    const onInterviewerStopSpeech = () => {
      console.log("INSIDE INTERVIEWER STOP")
      if (finished_intro==false) {
        grader.TextToSpeechFeature.play(grader_greeting)
      }
      if (during_interview==true) {
        // start the silence interval countdown
        toggleCountdown("start");
      }
    } 

    const onGraderStopSpeech = () => {
      if (finished_intro==false) {
        createInterviewRoom();
      }
    }

    const interviewer = speakers.get(host_name)
    const grader = speakers.get('Alien')

    interviewer.TextToSpeechFeature.play(greeting);
    interviewer.listenTo( interviewer.TextToSpeechFeature.EVENTS.stop,onInterviewerStopSpeech)
    grader.listenTo(grader.TextToSpeechFeature.EVENTS.stop, onGraderStopSpeech)

    // Skip intro button
    $('#skip-intro').click(function(){
      finished_intro = true;
      const {name, host} = getCurrentHost();
      host.TextToSpeechFeature.stop();
      createInterviewRoom();
    });

  }

  function initializeOutro() {

    toggleVisibility('.recButton');
    toggleVisibility('.stopInterviewButton');

    const outro_scene = intro_settings.get("scene");
    const saved_camera = intro_settings.get("camera");
    const grader = intro_settings.get("grader_chara");
    const interviewer = intro_settings.get("interviewer_chara");
    outro_scene.add(grader);
    outro_scene.remove(interviewer);
    grader.position.set(0, 0, 0);
    grader.rotateY(0);
    const camera = saved_camera.clone();  // Cloning the saved camera to avoid modifying the original

    // Renderer
    const renderer = new THREE.WebGLRenderer({antialias: true});
    renderer.setPixelRatio(window.devicePixelRatio);
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.outputEncoding = THREE.sRGBEncoding;
    renderer.shadowMap.enabled = true;
    renderer.setClearColor(0x33334d);
    renderer.domElement.id = 'renderCanvas';
    document.body.appendChild(renderer.domElement);

    const controls = new OrbitControls(camera, renderer.domElement);
    camera.position.set(0, 1.4, 3.1);
    controls.target = new THREE.Vector3(0, 0.8, 0);
    controls.screenSpacePanning = true;
    controls.update();
        // Render loop
    function render() {
      requestAnimationFrame(render);
      controls.update();
      renderFn.forEach(fn => {
        fn();
      });

      renderer.render(outro_scene, camera);
    }
    render();
  }


  function createInterviewRoom() {

    //remove intro scene
    const interview_scene = intro_settings.get("scene");
    const saved_camera = intro_settings.get('camera');
    const camera = saved_camera.clone();  // Cloning the saved camera to avoid modifying the original

    const grader = intro_settings.get("grader_chara");
    const interviewer = intro_settings.get('interviewer_chara');
    interview_scene.remove(grader);
    interviewer.position.set(-1.25, 0, 0);
    interviewer.rotateY(1);
    // const clock = intro_settings.get("clock")
    interview_scene.background = new THREE.Color(0x87ceeb); // Light blue color
    interview_scene.fog = new THREE.Fog(0x87ceeb, 0, 10);
     // Renderer
     const renderer = new THREE.WebGLRenderer({antialias: true});
    renderer.setPixelRatio(window.devicePixelRatio);
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.outputEncoding = THREE.sRGBEncoding;
    renderer.shadowMap.enabled = true;
    renderer.setClearColor(0x33334d);
    renderer.domElement.id = 'renderCanvas';
    document.body.appendChild(renderer.domElement);

    // Env map
    new THREE.TextureLoader()
      .setPath('./assets/')
      .load('images/background.png', hdrEquirect => {
        const hdrCubeRenderTarget = pmremGenerator.fromEquirectangular(
          hdrEquirect
        );
        hdrEquirect.dispose();
        pmremGenerator.dispose();

        interview_scene.environment = hdrCubeRenderTarget.texture;
      });

    const pmremGenerator = new THREE.PMREMGenerator(renderer);
    pmremGenerator.compileEquirectangularShader();

    //  // Camera
    //  const camera = new THREE.PerspectiveCamera(
    //   THREE.MathUtils.radToDeg(0.8),
    //   window.innerWidth / window.innerHeight,
    //   0.1,
    //   1000
    // );
    const controls = new OrbitControls(camera, renderer.domElement);
    camera.position.set(0, 1.4, 3.1);
    controls.target = new THREE.Vector3(0, 0.8, 0);
    controls.screenSpacePanning = true;
    controls.update();

    // Render loop
    function render() {
      requestAnimationFrame(render);
      controls.update();

      renderFn.forEach(fn => {
        fn();
      });

      renderer.render(interview_scene, camera);
    }

    render();

    // Lights
    const hemiLight = new THREE.HemisphereLight(0xffffff, 0x000000, 0.6);
    hemiLight.position.set(0, 1, 0);
    hemiLight.intensity = 0.6;
    interview_scene.add(hemiLight);

    const dirLight = new THREE.DirectionalLight(0xffffff);
    dirLight.position.set(0, 5, 5);

    dirLight.castShadow = true;
    dirLight.shadow.mapSize.width = 1024;
    dirLight.shadow.mapSize.height = 1024;
    dirLight.shadow.camera.top = 2.5;
    dirLight.shadow.camera.bottom = -2.5;
    dirLight.shadow.camera.left = -2.5;
    dirLight.shadow.camera.right = 2.5;
    dirLight.shadow.camera.near = 0.1;
    dirLight.shadow.camera.far = 40;
    interview_scene.add(dirLight);

    const dirLightTarget = new THREE.Object3D();
    dirLight.add(dirLightTarget);
    dirLightTarget.position.set(0, -0.5, -1.0);
    dirLight.target = dirLightTarget;

    // Environment
    const groundMat = new THREE.MeshStandardMaterial({
      color: 0x808080,
      depthWrite: false,
    });
    groundMat.metalness = 0;
    groundMat.refractionRatio = 0;
    const ground = new THREE.Mesh(
      new THREE.PlaneBufferGeometry(100, 100),
      groundMat
    );
    ground.rotation.x = -Math.PI / 2;
    ground.receiveShadow = true;
    interview_scene.add(ground);

    initializeUX();

    // return {interview_scene, camera, clock};


  }

  

  // Set up base scene
  function createScene() {
    // Base scene
    const scene = new THREE.Scene();
    const clock = new THREE.Clock();
    scene.background = new THREE.Color(0x33334d);
    scene.fog = new THREE.Fog(0x33334d, 0, 10);

    // Renderer
    const renderer = new THREE.WebGLRenderer({antialias: true});
    renderer.setPixelRatio(window.devicePixelRatio);
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.outputEncoding = THREE.sRGBEncoding;
    renderer.shadowMap.enabled = true;
    renderer.setClearColor(0x33334d);
    renderer.domElement.id = 'renderCanvas';
    document.body.appendChild(renderer.domElement);

    // Env map
    new THREE.TextureLoader()
      .setPath('./assets/')
      .load('images/machine_shop.jpg', hdrEquirect => {
        const hdrCubeRenderTarget = pmremGenerator.fromEquirectangular(
          hdrEquirect
        );
        hdrEquirect.dispose();
        pmremGenerator.dispose();

        scene.environment = hdrCubeRenderTarget.texture;
      });

    const pmremGenerator = new THREE.PMREMGenerator(renderer);
    pmremGenerator.compileEquirectangularShader();

    // Camera
    const camera = new THREE.PerspectiveCamera(
      THREE.MathUtils.radToDeg(0.8),
      window.innerWidth / window.innerHeight,
      0.1,
      1000
    );
    const controls = new OrbitControls(camera, renderer.domElement);
    camera.position.set(0, 1.4, 3.1);
    controls.target = new THREE.Vector3(0, 0.8, 0);
    controls.screenSpacePanning = true;
    controls.update();

    // Handle window resize
    function onWindowResize() {
      camera.aspect = window.innerWidth / window.innerHeight;
      camera.updateProjectionMatrix();
      renderer.setSize(window.innerWidth, window.innerHeight);
    }
    window.addEventListener('resize', onWindowResize, false);

    // Render loop
    function render() {
      requestAnimationFrame(render);
      controls.update();

      renderFn.forEach(fn => {
        fn();
      });

      renderer.render(scene, camera);
    }

    render();

    // Lights
    const hemiLight = new THREE.HemisphereLight(0xffffff, 0x000000, 0.6);
    hemiLight.position.set(0, 1, 0);
    hemiLight.intensity = 0.6;
    scene.add(hemiLight);

    const dirLight = new THREE.DirectionalLight(0xffffff);
    dirLight.position.set(0, 5, 5);

    dirLight.castShadow = true;
    dirLight.shadow.mapSize.width = 1024;
    dirLight.shadow.mapSize.height = 1024;
    dirLight.shadow.camera.top = 2.5;
    dirLight.shadow.camera.bottom = -2.5;
    dirLight.shadow.camera.left = -2.5;
    dirLight.shadow.camera.right = 2.5;
    dirLight.shadow.camera.near = 0.1;
    dirLight.shadow.camera.far = 40;
    scene.add(dirLight);

    const dirLightTarget = new THREE.Object3D();
    dirLight.add(dirLightTarget);
    dirLightTarget.position.set(0, -0.5, -1.0);
    dirLight.target = dirLightTarget;

    // Environment
    const groundMat = new THREE.MeshStandardMaterial({
      color: 0x808080,
      depthWrite: false,
    });
    groundMat.metalness = 0;
    groundMat.refractionRatio = 0;
    const ground = new THREE.Mesh(
      new THREE.PlaneBufferGeometry(100, 100),
      groundMat
    );
    ground.rotation.x = -Math.PI / 2;
    ground.receiveShadow = true;
    scene.add(ground);

    return {scene, camera, clock};
  }

  async function getCharacterData(characterName) {
    try {
      const response = await fetch('./assets/character_mapping.json');
      const jsonData = await response.json();
      return jsonData[characterName];
    } catch (error) {
      console.error('Error loading JSON:', error);
      return null;
    }
  }
  
  // Load character model and animations
  async function loadCharacter(
    scene,
    characterFile,
    animationPath,
    animationFiles
  ) {
    // Asset loader
    const fileLoader = new THREE.FileLoader();
    const gltfLoader = new GLTFLoader();

    function loadAsset(loader, assetPath, onLoad) {
      return new Promise(resolve => {
        loader.load(assetPath, async asset => {
          if (onLoad[Symbol.toStringTag] === 'AsyncFunction') {
            const result = await onLoad(asset);
            resolve(result);
          } else {
            resolve(onLoad(asset));
          }
        });
      });
    }

    // Load character model
    const {character, bindPoseOffset} = await loadAsset(
      gltfLoader,
      characterFile,
      gltf => {
        // Transform the character
        const character = gltf.scene;
        scene.add(character);

        // Make the offset pose additive
        const [bindPoseOffset] = gltf.animations;
        if (bindPoseOffset) {
          THREE.AnimationUtils.makeClipAdditive(bindPoseOffset);
        }

        // Cast shadows
        character.traverse(object => {
          if (object.isMesh) {
            object.castShadow = true;
          }
        });

        return {character, bindPoseOffset};
      }
    );

    // Load animations
    const clips = await Promise.all(
      animationFiles.map((filename, index) => {
        const filePath = `${animationPath}/${filename}`;

        return loadAsset(gltfLoader, filePath, async gltf => {
          return gltf.animations;
        });
      })
    );

    return {character, clips, bindPoseOffset};
  }

  // Initialize the host
  function createHost(
    character,
    audioAttachJoint,
    voice,
    engine,
    idleClip,
    faceIdleClip,
    lipsyncClips,
    gestureClips,
    gestureConfig,
    emoteClips,
    blinkClips,
    poiClips,
    poiConfig,
    lookJoint,
    bindPoseOffset,
    clock,
    camera,
    scene
  ) {
    // Add the host to the render loop
    const host = new HOST.HostObject({owner: character, clock});
    renderFn.push(() => {
      host.update();
    });

    // Set up text to speech
    const audioListener = new THREE.AudioListener();
    camera.add(audioListener);
    host.addFeature(HOST.aws.TextToSpeechFeature, false, {
      listener: audioListener,
      attachTo: audioAttachJoint,
      voice,
      engine,
    });

    // Set up animation
    host.addFeature(HOST.anim.AnimationFeature);

    // Base idle
    host.AnimationFeature.addLayer('Base');
    host.AnimationFeature.addAnimation(
      'Base',
      idleClip.name,
      HOST.anim.AnimationTypes.single,
      {clip: idleClip}
    );
    host.AnimationFeature.playAnimation('Base', idleClip.name);

    // Face idle
    host.AnimationFeature.addLayer('Face', {
      blendMode: HOST.anim.LayerBlendModes.Additive,
    });
    THREE.AnimationUtils.makeClipAdditive(faceIdleClip);
    host.AnimationFeature.addAnimation(
      'Face',
      faceIdleClip.name,
      HOST.anim.AnimationTypes.single,
      {
        clip: THREE.AnimationUtils.subclip(
          faceIdleClip,
          faceIdleClip.name,
          1,
          faceIdleClip.duration * 30,
          30
        ),
      }
    );
    host.AnimationFeature.playAnimation('Face', faceIdleClip.name);

    // Blink
    host.AnimationFeature.addLayer('Blink', {
      blendMode: HOST.anim.LayerBlendModes.Additive,
      transitionTime: 0.075,
    });
    blinkClips.forEach(clip => {
      THREE.AnimationUtils.makeClipAdditive(clip);
    });
    host.AnimationFeature.addAnimation(
      'Blink',
      'blink',
      HOST.anim.AnimationTypes.randomAnimation,
      {
        playInterval: 3,
        subStateOptions: blinkClips.map(clip => {
          return {
            name: clip.name,
            loopCount: 1,
            clip,
          };
        }),
      }
    );
    host.AnimationFeature.playAnimation('Blink', 'blink');

    // Talking idle
    host.AnimationFeature.addLayer('Talk', {
      transitionTime: 0.75,
      blendMode: HOST.anim.LayerBlendModes.Additive,
    });
    host.AnimationFeature.setLayerWeight('Talk', 0);
    const talkClip = lipsyncClips.find(c => c.name === 'stand_talk');
    lipsyncClips.splice(lipsyncClips.indexOf(talkClip), 1);
    host.AnimationFeature.addAnimation(
      'Talk',
      talkClip.name,
      HOST.anim.AnimationTypes.single,
      {clip: THREE.AnimationUtils.makeClipAdditive(talkClip)}
    );
    host.AnimationFeature.playAnimation('Talk', talkClip.name);

    // Gesture animations
    host.AnimationFeature.addLayer('Gesture', {
      transitionTime: 0.5,
      blendMode: HOST.anim.LayerBlendModes.Additive,
    });
    gestureClips.forEach(clip => {
      const {name} = clip;
      const config = gestureConfig[name];
      THREE.AnimationUtils.makeClipAdditive(clip);

      if (config !== undefined) {
        config.queueOptions.forEach((option, index) => {
          // Create a subclip for each range in queueOptions
          option.clip = THREE.AnimationUtils.subclip(
            clip,
            `${name}_${option.name}`,
            option.from,
            option.to,
            30
          );
        });
        host.AnimationFeature.addAnimation(
          'Gesture',
          name,
          HOST.anim.AnimationTypes.queue,
          config
        );
      } else {
        host.AnimationFeature.addAnimation(
          'Gesture',
          name,
          HOST.anim.AnimationTypes.single,
          {clip}
        );
      }
    });

    // Emote animations
    host.AnimationFeature.addLayer('Emote', {
      transitionTime: 0.5,
    });

    emoteClips.forEach(clip => {
      const {name} = clip;
      host.AnimationFeature.addAnimation(
        'Emote',
        name,
        HOST.anim.AnimationTypes.single,
        {clip, loopCount: 1}
      );
    });

    // Viseme poses
    host.AnimationFeature.addLayer('Viseme', {
      transitionTime: 0.12,
      blendMode: HOST.anim.LayerBlendModes.Additive,
    });
    host.AnimationFeature.setLayerWeight('Viseme', 0);

    // Slice off the reference frame
    const blendStateOptions = lipsyncClips.map(clip => {
      THREE.AnimationUtils.makeClipAdditive(clip);
      return {
        name: clip.name,
        clip: THREE.AnimationUtils.subclip(clip, clip.name, 1, 2, 30),
        weight: 0,
      };
    });
    host.AnimationFeature.addAnimation(
      'Viseme',
      'visemes',
      HOST.anim.AnimationTypes.freeBlend,
      {blendStateOptions}
    );
    host.AnimationFeature.playAnimation('Viseme', 'visemes');

    // POI poses
    poiConfig.forEach(config => {
      host.AnimationFeature.addLayer(config.name, {
        blendMode: HOST.anim.LayerBlendModes.Additive,
      });

      // Find each pose clip and make it additive
      config.blendStateOptions.forEach(clipConfig => {
        const clip = poiClips.find(clip => clip.name === clipConfig.clip);
        THREE.AnimationUtils.makeClipAdditive(clip);
        clipConfig.clip = THREE.AnimationUtils.subclip(
          clip,
          clip.name,
          1,
          2,
          30
        );
      });

      host.AnimationFeature.addAnimation(
        config.name,
        config.animation,
        HOST.anim.AnimationTypes.blend2d,
        {...config}
      );

      host.AnimationFeature.playAnimation(config.name, config.animation);

      // Find and store reference objects
      config.reference = character.getObjectByName(
        config.reference.replace(':', '')
      );
    });

    // Apply bindPoseOffset clip if it exists
    if (bindPoseOffset !== undefined) {
      host.AnimationFeature.addLayer('BindPoseOffset', {
        blendMode: HOST.anim.LayerBlendModes.Additive,
      });
      host.AnimationFeature.addAnimation(
        'BindPoseOffset',
        bindPoseOffset.name,
        HOST.anim.AnimationTypes.single,
        {
          clip: THREE.AnimationUtils.subclip(
            bindPoseOffset,
            bindPoseOffset.name,
            1,
            2,
            30
          ),
        }
      );
      host.AnimationFeature.playAnimation(
        'BindPoseOffset',
        bindPoseOffset.name
      );
    }

    // Set up Lipsync
    const visemeOptions = {
      layers: [{name: 'Viseme', animation: 'visemes'}],
    };
    const talkingOptions = {
      layers: [
        {
          name: 'Talk',
          animation: 'stand_talk',
          blendTime: 0.75,
          easingFn: HOST.anim.Easing.Quadratic.InOut,
        },
      ],
    };
    host.addFeature(
      HOST.LipsyncFeature,
      false,
      visemeOptions,
      talkingOptions
    );

    // Set up Gestures
    host.addFeature(HOST.GestureFeature, false, {
      layers: {
        Gesture: {minimumInterval: 3},
        Emote: {
          blendTime: 0.5,
          easingFn: HOST.anim.Easing.Quadratic.InOut,
        },
      },
    });

    // Set up Point of Interest
    host.addFeature(
      HOST.PointOfInterestFeature,
      false,
      {
        target: camera,
        lookTracker: lookJoint,
        scene,
      },
      {
        layers: poiConfig,
      },
      {
        layers: [{name: 'Blink'}],
      }
    );

    return host;
  }

  // Return the host whose name matches the text of the current tab
  function getCurrentHost() {
    const tab = document.getElementsByClassName('tab current')[0];
    const name = tab.textContent;
    return {name, host: speakers.get(name)};
  }

  function setCurrentHost(name) {
    const tab = document.getElementsByClassName('tab current')[0];
    tab.textContent = name;
  }



  // Update UX with data for the current host
  function toggleHost(evt) {
    // const tab = evt.target;
    const tab = document.getElementsByClassName('tab current')[0];
    const allTabs = document.getElementsByClassName('tab');

    // Update tab classes
    for (let i = 0, l = allTabs.length; i < l; i++) {
      if (allTabs[i] !== tab) {
        allTabs[i].classList.remove('current');
      } else {
        allTabs[i].classList.add('current');
      }
    }

    // Show/hide speech input classes
    // const {name, host} = getCurrentHost(speakers);
    // const textEntries = document.getElementsByClassName('textEntry');

    // for (let i = 0, l = textEntries.length; i < l; i += 1) {
    //   const textEntry = textEntries[i];

    //   if (textEntry.classList.contains(name)) {
    //     textEntry.classList.remove('hidden');
    //   } else {
    //     textEntry.classList.add('hidden');
    //   }
    // }

    // Update emote selector
    // const emoteSelect = document.getElementById('emotes');
    // emoteSelect.length = 0;
    // const emotes = host.AnimationFeature.getAnimations('Emote');
    // emotes.forEach((emote, i) => {
    //   const emoteOption = document.createElement('option');
    //   emoteOption.text = emote;
    //   emoteOption.value = emote;
    //   emoteSelect.add(emoteOption, 0);

    //   // Set the current item to the first emote
    //   if (!i) {
    //     emoteSelect.value = emote;
    //   }
    // });
  }


  // Function to start recording timer
  function toggleRecTimer(mode, seconds=0) {
    console.log("recording timer started");
      // updateTimerDisplay();
    if (mode=="start") {
      timerInterval = setInterval(function () {
          seconds++;
          document.getElementById('recDisplay').innerHTML = `${seconds} s`;
            // updateTimerDisplay(seconds);
        }, 1000);
    }
    else if (mode=="stop") {
      document.getElementById('recDisplay').innerHTML = `REC`;
      clearInterval(timerInterval)
    }
  }
 // Function to format countdown timer's display
  function formatTimeLeft(time) {
      // The largest round integer less than or equal to the result of time divided being by 60.
      const minutes = Math.floor(time / 60);    
      // Seconds are the remainder of the time divided by 60 (modulus operator)
      let seconds = time % 60;     
      // If the value of seconds is less than 10, then display seconds with a leading zero
      if (seconds < 10) {
        seconds = `0${seconds}`;
      }
      // The output in MM:SS format
      return `${minutes}:${seconds}`;
    }
  // Function to start a countdown timer
  function toggleCountdown(mode, TIME_LIMIT=20, timePassed=0) {
    if (mode=="start") {
        toggleVisibility('.base-timer');
        countdownInterval = setInterval(() => { 
        // The amount of time passed increments by one
        timePassed = timePassed += 1;
        // var timeLeft = TIME_LIMIT - timePassed; 
        // The time left label is updated
        document.getElementById("base-timer-label").textContent = formatTimeLeft(TIME_LIMIT - timePassed);
        // setCircleDasharray(TIME_LIMIT, timeLeft);
        }, 1000);
    }
    else if (mode=="stop") {
      if (count>=1) {
        console.log("toggling base timer")
        toggleVisibility('.base-timer');
        clearInterval(countdownInterval);
      }
      count++;
    }
  }



  // Divides time left by the defined time limit.
  // function calculateTimeFraction() {
  //   return timeLeft / TIME_LIMIT;
  // }
  // Update the dasharray value as time passes, starting with 283
  // function setCircleDasharray(TIME_LIMIT, timeLeft) {
  //   const circleDasharray = `${(
  //     timeLeft/TIME_LIMIT * FULL_DASH_ARRAY
  //   ).toFixed(0)} 283`;
  //   document
  //     .getElementById("base-timer-path-remaining")
  //     .setAttribute("stroke-dasharray", circleDasharray);
  // }textEntry


function toggleVisibility(classname) {
  const recButton = document.querySelector(classname);
  recButton.classList.toggle('hidden');
}




  function initializeUX(speakers) {

    // Enable drag/drop text files on the speech text area
    // setCurrentHost(host_name);
    enableDragDrop('notePad');
    toggleVisibility('.recButton');
    toggleVisibility('.skipIntroButton');
    toggleVisibility('.stopInterviewButton');
    finished_intro=true;
    during_interview=true;
  

    // Connect tab buttons to hosts
    // Array.from(document.getElementsByClassName('tab')).forEach(tab => {
    //   tab.onclick = evt => { toggleHost(evt); }
    // });
    $('.recButton').addClass("notRec");
    $('.recButton').click(function(){
      if($('.recButton').hasClass('notRec')){
        $('.recButton').removeClass("notRec");
        $('.recButton').addClass("Rec");
        // stop the silence interval countdown
        toggleCountdown("stop");
        // start the recording timer
        toggleRecTimer("start");
        audioRecorder.startRecording();
      }
      else{
        $('.recButton').removeClass("Rec");
        $('.recButton').addClass("notRec");
        // stop the recording timer
        toggleRecTimer("stop");
        audioRecorder.stopRecording();

      }
    });	

    $('.stopInterviewButton').click(function() {
      initializeOutro();
    });


    // Play, pause, resume and stop the contents of the text input as speech
    // when buttons are clicked
    // ['play', 'pause', 'resume', 'stop'].forEach(id => {
    //   const button = document.getElementById(id);
    //   button.onclick = () => {
    //     const {name, host} = getCurrentHost(speakers);
    //     const speechInput = document.getElementsByClassName(
    //       `textEntry ${name}`
    //     )[0];
    //     host.TextToSpeechFeature[id](speechInput.value);
    //   };
    // });
    // const {name, host} = getCurrentHost();
    // console.log(name, host)
    const host = intro_settings.get("interviewer_host")

    // Set up speech generation 
    const textareaElement = document.getElementsByClassName('textEntry Interviewer')[0];
    console.log(textareaElement.value);
    textareaElement.addEventListener("speechContentUpdated", evt => {
        evt.stopPropagation();
        console.log("text area is changed:", textareaElement.value);
        const gestureMap = host.GestureFeature.createGestureMap();
        const gestureArray = host.GestureFeature.createGenericGestureArray([
          'Gesture',
        ]);
        textareaElement.value = HOST.aws.TextToSpeechUtils.autoGenerateSSMLMarks(
          textareaElement.value,
          gestureMap,
          gestureArray
          );
        host.TextToSpeechFeature.play(textareaElement.value);
    });

  }

  



  function enableDragDrop(className) {
    const elements = document.getElementsByClassName(className);

    for (let i = 0, l = elements.length; i < l; i += 1) {
      const dropArea = elements[i];

      // Copy contents of files into the text input once they are read
      const fileReader = new FileReader();
      fileReader.onload = evt => {
        dropArea.value = evt.target.result;
      };

      // Drag and drop listeners
      dropArea.addEventListener('dragover', evt => {
        evt.stopPropagation();
        evt.preventDefault();
        evt.dataTransfer.dropEffect = 'copy';
      });

      dropArea.addEventListener('drop', evt => {
        evt.stopPropagation();
        evt.preventDefault();

        // Read the first file that was dropped
        const [file] = evt.dataTransfer.files;
        fileReader.readAsText(file, 'UTF-8');
      });
    }
  }
</script>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
    <!-- </div> -->
  </body>
</html>
